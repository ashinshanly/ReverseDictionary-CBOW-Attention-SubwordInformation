{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CBOW.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashinshanly/ReverseDictionary-Using-CBOW-with-Attention-and-subword-information/blob/main/CBOW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKRwpqi_XHQl",
        "outputId": "f7588c17-1fc9-4b02-d58c-58530b26243d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J00Za-nTAw-o"
      },
      "source": [
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Reshape, Flatten, LSTM, Dense, Dropout, Embedding, Bidirectional, GRU\n",
        "from keras.optimizers import Adam\n",
        "from keras import initializers, regularizers\n",
        "from keras import optimizers\n",
        "from keras.engine.topology import Layer\n",
        "from keras import constraints\n",
        "def dot_product(x, kernel):\n",
        "\t\"\"\"\n",
        "\tWrapper for dot product operation, in order to be compatible with both\n",
        "\tTheano and Tensorflow\n",
        "\tArgs:\n",
        "\t\tx (): input\n",
        "\t\tkernel (): weights\n",
        "\tReturns:\n",
        "\t\"\"\"\n",
        "\tif K.backend() == 'tensorflow':\n",
        "\t\treturn K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
        "\telse:\n",
        "\t\treturn K.dot(x, kernel)\n",
        "\n",
        "class AttentionWithContext(keras.layers.Layer):\n",
        "\t\"\"\"\n",
        "\tAttention operation, with a context/query vector, for temporal data.\n",
        "\tSupports Masking.\n",
        "\tfollows these equations:\n",
        "\t\n",
        "\t(1) u_t = tanh(W h_t + b)\n",
        "\t(2) \\alpha_t = \\frac{exp(u^T u)}{\\sum_t(exp(u_t^T u))}, this is the attention weight\n",
        "\t(3) v_t = \\alpha_t * h_t, v in time t\n",
        "\t# Input shape\n",
        "\t\t3D tensor with shape: `(samples, steps, features)`.\n",
        "\t# Output shape\n",
        "\t\t3D tensor with shape: `(samples, steps, features)`.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self,\n",
        "\t\t\t\t W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
        "\t\t\t\t W_constraint=None, u_constraint=None, b_constraint=None,\n",
        "\t\t\t\t bias=True, **kwargs):\n",
        "\n",
        "\t\tself.supports_masking = True\n",
        "\t\tself.init = initializers.get('glorot_uniform')\n",
        "\n",
        "\t\tself.W_regularizer = regularizers.get(W_regularizer)\n",
        "\t\tself.u_regularizer = regularizers.get(u_regularizer)\n",
        "\t\tself.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "\t\tself.W_constraint = constraints.get(W_constraint)\n",
        "\t\tself.u_constraint = constraints.get(u_constraint)\n",
        "\t\tself.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "\t\tself.bias = bias\n",
        "\t\tsuper(AttentionWithContext, self).__init__(**kwargs)\n",
        "\n",
        "\tdef build(self, input_shape):\n",
        "\t\tassert len(input_shape) == 3\n",
        "\n",
        "\t\tself.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
        "\t\t\t\t\t\t\t\t initializer=self.init,\n",
        "\t\t\t\t\t\t\t\t name='{}_W'.format(self.name),\n",
        "\t\t\t\t\t\t\t\t regularizer=self.W_regularizer,\n",
        "\t\t\t\t\t\t\t\t constraint=self.W_constraint)\n",
        "\t\tif self.bias:\n",
        "\t\t\tself.b = self.add_weight(shape=(input_shape[-1]),\n",
        "\t\t\t\t\t\t\t\t\t initializer='zero',\n",
        "\t\t\t\t\t\t\t\t\t name='{}_b'.format(self.name),\n",
        "\t\t\t\t\t\t\t\t\t regularizer=self.b_regularizer,\n",
        "\t\t\t\t\t\t\t\t\t constraint=self.b_constraint)\n",
        "\n",
        "\t\tself.u = self.add_weight(shape = (input_shape[-1],),\n",
        "\t\t\t\t\t\t\t\t initializer=self.init,\n",
        "\t\t\t\t\t\t\t\t name='{}_u'.format(self.name),\n",
        "\t\t\t\t\t\t\t\t regularizer=self.u_regularizer,\n",
        "\t\t\t\t\t\t\t\t constraint=self.u_constraint)\n",
        "\n",
        "\t\tsuper(AttentionWithContext, self).build(input_shape)\n",
        "\n",
        "\tdef compute_mask(self, input, input_mask=None):\n",
        "\t\t# do not pass the mask to the next layers\n",
        "\t\treturn None\n",
        "\n",
        "\tdef call(self, x, mask=None):\n",
        "\t\tuit = dot_product(x, self.W)\n",
        "\n",
        "\t\tif self.bias:\n",
        "\t\t\tuit += self.b\n",
        "\n",
        "\t\tuit = K.tanh(uit)\n",
        "\t\tait = dot_product(uit, self.u)\n",
        "\n",
        "\t\ta = K.exp(ait)\n",
        "\n",
        "\t\t# apply mask after the exp. will be re-normalized next\n",
        "\t\tif mask is not None:\n",
        "\t\t\t# Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "\t\t\ta *= K.cast(mask, K.floatx())\n",
        "\n",
        "\t\t# in some cases especially in the early stages of training the sum may be almost zero and this results in NaN's. \n",
        "\t\t# Should add a small epsilon as the workaround\n",
        "\t\t# a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
        "\t\ta /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "\t\ta = K.expand_dims(a)\n",
        "\t\tweighted_input = x * a\n",
        "\t\t\n",
        "\t\treturn weighted_input\n",
        "\n",
        "\tdef compute_output_shape(self, input_shape):\n",
        "\t\treturn input_shape[0], input_shape[1], input_shape[2]\n",
        "\t\n",
        "class Addition(Layer):\n",
        "\t\"\"\"\n",
        "\tThis layer is supposed to add of all activation weight.\n",
        "\tWe split this from AttentionWithContext to help us getting the activation weights\n",
        "\tfollows this equation:\n",
        "\t(1) v = \\sum_t(\\alpha_t * h_t)\n",
        "\t\n",
        "\t# Input shape\n",
        "\t\t3D tensor with shape: `(samples, steps, features)`.\n",
        "\t# Output shape\n",
        "\t\t2D tensor with shape: `(samples, features)`.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, **kwargs):\n",
        "\t\tsuper(Addition, self).__init__(**kwargs)\n",
        "\n",
        "\tdef build(self, input_shape):\n",
        "\t\tself.output_dim = input_shape[-1]\n",
        "\t\tsuper(Addition, self).build(input_shape)\n",
        "\n",
        "\tdef call(self, x):\n",
        "\t\treturn K.sum(x, axis=1)\n",
        "\n",
        "\tdef compute_output_shape(self, input_shape):\n",
        "\t\treturn (input_shape[0], self.output_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQb2QmDBNmeH",
        "outputId": "9bdc377a-c462-43a8-a2fd-e2870a9509a5"
      },
      "source": [
        "from keras.preprocessing import text\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "#data = open(\"/content/drive/MyDrive/ReverseDictionary/data/dataset_final.txt\",\"r\")\n",
        "data = open(\"/content/drive/MyDrive/ReverseDictionary/data/dataset9.txt\",\"r\")\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data)\n",
        "word2id = tokenizer.word_index\n",
        "\n",
        "# build vocabulary of unique words\n",
        "word2id['PAD'] = 0\n",
        "id2word = {v:k for k, v in word2id.items()}\n",
        "#with open('/content/drive/MyDrive/ReverseDictionary/data/dataset_final.txt', 'r') as data:\n",
        "#    wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in data]\n",
        "with open('/content/drive/MyDrive/ReverseDictionary/data/dataset9.txt', 'r') as data:\n",
        "    wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in data]\n",
        "\n",
        "#print(wids)\n",
        "vocab_size = len(word2id)\n",
        "embed_size = 100\n",
        "window_size = 2 # context window size\n",
        "\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Vocabulary Sample:', list(word2id.items())[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 5799\n",
            "Vocabulary Sample: [('something', 1), ('person', 2), ('especially', 3), ('used', 4), ('someone', 5), ('typically', 6), ('action', 7), ('particular', 8), ('people', 9), ('part', 10), ('place', 11), ('time', 12), ('thing', 13), ('state', 14), ('relating', 15), ('ones', 16), ('number', 17), ('body', 18), ('things', 19), ('make', 20), ('made', 21), ('process', 22), ('another', 23), ('large', 24), ('object', 25), ('surface', 26), ('area', 27), ('animal', 28), ('form', 29), ('fact', 30), ('order', 31), ('amount', 32), ('move', 33), ('small', 34), ('group', 35), ('piece', 36), ('material', 37), ('give', 38), ('event', 39), ('done', 40), ('activity', 41), ('position', 42), ('period', 43), ('without', 44), ('feeling', 45), ('great', 46), ('money', 47), ('point', 48), ('food', 49), ('physical', 50), ('cause', 51), ('together', 52), ('quality', 53), ('building', 54), ('substance', 55), ('public', 56), ('long', 57), ('degree', 58), ('human', 59), ('condition', 60), ('work', 61), ('extent', 62), ('others', 63), ('similar', 64), ('purpose', 65), ('etc', 66), ('organization', 67), ('result', 68), ('size', 69), ('present', 70), ('specified', 71), ('refer', 72), ('different', 73), ('take', 74), ('information', 75), ('formal', 76), ('living', 77), ('whole', 78), ('consisting', 79), ('parts', 80), ('space', 81), ('side', 82), ('situation', 83), ('mentioned', 84), ('power', 85), ('way', 86), ('natural', 87), ('occurring', 88), ('written', 89), ('regarded', 90), ('persons', 91), ('liquid', 92), ('kind', 93), ('plant', 94), ('direction', 95), ('matter', 96), ('considered', 97), ('goods', 98), ('water', 99), ('ground', 100)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIQ5L_uSPb8h",
        "outputId": "57e0e4f8-f6e1-4ec4-d3b5-c11240cdb997"
      },
      "source": [
        "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
        "    # print(corpus)\n",
        "    context_length = window_size*2\n",
        "    for words in corpus:\n",
        "        sentence_length = len(words)\n",
        "        for index, word in enumerate(words):\n",
        "            context_words = []\n",
        "            label_word   = []            \n",
        "            start = index - window_size\n",
        "            end = index + window_size + 1            \n",
        "            context_words.append([words[i] \n",
        "                                 for i in range(start, end) \n",
        "                                 if 0 <= i < sentence_length \n",
        "                                 and i != index])\n",
        "            label_word.append(word)\n",
        "            # context_words.append([words[1:]])\n",
        "            # label_word.append(words[0])\n",
        "            x = sequence.pad_sequences(context_words, maxlen = context_length)\n",
        "            y = np_utils.to_categorical( label_word, vocab_size )\n",
        "            yield (x, y)\n",
        "            \n",
        "# Test this out for some samples\n",
        "i = 0\n",
        "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
        "    print(x,y)\n",
        "    # print(y)\n",
        "    if 0 not in x[0]:\n",
        "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
        "    \n",
        "        if i == 2:\n",
        "            break\n",
        "        i += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0   0 600 145]] [[0. 0. 0. ... 0. 0. 0.]]\n",
            "[[   0 1816  145  261]] [[0. 0. 0. ... 0. 0. 0.]]\n",
            "[[1816  600  261    5]] [[0. 0. 0. ... 0. 0. 0.]]\n",
            "Context (X): ['abandon', 'cease', 'look', 'someone'] -> Target (Y): support\n",
            "[[ 600  145    5 1817]] [[0. 0. 0. ... 0. 0. 0.]]\n",
            "Context (X): ['cease', 'support', 'someone', 'desert'] -> Target (Y): look\n",
            "[[   0  145  261 1817]] [[0. 0. 0. ... 0. 0. 0.]]\n",
            "[[  0   0 261   5]] [[0. 0. 0. ... 0. 0. 0.]]\n",
            "[[  0   0 601 129]] [[0. 0. 0. ... 0. 0. 0.]]\n",
            "[[  0 114 129 121]] [[0. 0. 0. ... 0. 0. 0.]]\n",
            "[[114 601 121   1]] [[0. 0. 0. ... 0. 0. 0.]]\n",
            "Context (X): ['ability', 'possession', 'skill', 'something'] -> Target (Y): means\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQEcp59o-j1z",
        "outputId": "64daf292-dbe8-467b-f158-f4bfa6619edc"
      },
      "source": [
        "def generate_context_word_pairs1(corpus, window_size, vocab_size):\n",
        "    # print(corpus)\n",
        "    context_length = window_size*2\n",
        "    context_words = []\n",
        "    label_word   = [] \n",
        "    for words in corpus:\n",
        "        sentence_length = len(words)\n",
        "        \n",
        "        context_words = []\n",
        "        label_word   = [] \n",
        "        context_words.append(words[1:])\n",
        "        label_word.append(words[0])\n",
        "        x = sequence.pad_sequences(context_words, maxlen=len(words)-1)\n",
        "        y = np_utils.to_categorical(label_word, vocab_size)\n",
        "        # print(x,y)\n",
        "        yield (x, y)\n",
        "\n",
        "i = 0\n",
        "for x, y in generate_context_word_pairs1(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
        "    # print(x,y)\n",
        "    # print(y)\n",
        "    if 0 not in x[0]:\n",
        "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
        "    \n",
        "        if i == 10:\n",
        "            break\n",
        "        i += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Context (X): ['cease', 'support', 'look', 'someone', 'desert'] -> Target (Y): abandon\n",
            "Context (X): ['possession', 'means', 'skill', 'something'] -> Target (Y): ability\n",
            "Context (X): ['power', 'skill', 'means', 'opportunity', 'something'] -> Target (Y): able\n",
            "Context (X): ['deliberate', 'termination', 'human', 'pregnancy', 'often', 'performed', 'first', 'weeks', 'pregnancy'] -> Target (Y): abortion\n",
            "Context (X): ['subject', 'concerning'] -> Target (Y): about\n",
            "Context (X): ['extended', 'space', 'touching'] -> Target (Y): above\n",
            "Context (X): ['foreign', 'country', 'countries'] -> Target (Y): abroad\n",
            "Context (X): ['state', 'away', 'place', 'person'] -> Target (Y): absence\n",
            "Context (X): ['qualified', 'diminished', 'way', 'total'] -> Target (Y): absolute\n",
            "Context (X): ['qualification', 'restriction', 'limitation', 'totally'] -> Target (Y): absolutely\n",
            "Context (X): ['take', 'soak', 'energy', 'liquid', 'substance', 'chemical', 'physical', 'action'] -> Target (Y): absorb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mOrO8v0dR_gP",
        "outputId": "15fa25bb-edd9-4fb1-b450-c55db8564e08"
      },
      "source": [
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda\n",
        "#from layers import AttentionWithContext,Addition\n",
        "\n",
        "# build CBOW architecture\n",
        "cbow = Sequential()\n",
        "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size))\n",
        "\n",
        "cbow.add(AttentionWithContext())\n",
        "# cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,0,0)))\n",
        "cbow.add(Addition())\n",
        "cbow.add(Dense(vocab_size, activation='softmax'))\n",
        "cbow.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Line to load the model again\n",
        "# cbow.load_weights(\"/content/drive/MyDrive/ReverseDictionary/models/weights-improvement-cbow.hdf5\")\n",
        "# view model summary\n",
        "print(cbow.summary())\n",
        "\n",
        "# visualize model structure\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, \n",
        "                 rankdir='TB').create(prog='dot', format='svg'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 100)         579900    \n",
            "_________________________________________________________________\n",
            "attention_with_context (Atte (None, None, 100)         10200     \n",
            "_________________________________________________________________\n",
            "addition (Addition)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5799)              585699    \n",
            "=================================================================\n",
            "Total params: 1,175,799\n",
            "Trainable params: 1,175,799\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<svg height=\"516pt\" viewBox=\"0.00 0.00 285.00 387.00\" width=\"380pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1.3333 1.3333) rotate(0) translate(4 383)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-383 281,-383 281,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140268213922336 -->\n<g class=\"node\" id=\"node1\">\n<title>140268213922336</title>\n<polygon fill=\"none\" points=\"42.5,-332.5 42.5,-378.5 234.5,-378.5 234.5,-332.5 42.5,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82.5\" y=\"-351.8\">InputLayer</text>\n<polyline fill=\"none\" points=\"122.5,-332.5 122.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"151.5\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"122.5,-355.5 180.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"151.5\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"180.5,-332.5 180.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"207.5\" y=\"-363.3\">[(?, ?)]</text>\n<polyline fill=\"none\" points=\"180.5,-355.5 234.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"207.5\" y=\"-340.3\">[(?, ?)]</text>\n</g>\n<!-- 140269074890824 -->\n<g class=\"node\" id=\"node2\">\n<title>140269074890824</title>\n<polygon fill=\"none\" points=\"30,-249.5 30,-295.5 247,-295.5 247,-249.5 30,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"72\" y=\"-268.8\">Embedding</text>\n<polyline fill=\"none\" points=\"114,-249.5 114,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"114,-272.5 172,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"172,-249.5 172,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"209.5\" y=\"-280.3\">(?, ?)</text>\n<polyline fill=\"none\" points=\"172,-272.5 247,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"209.5\" y=\"-257.3\">(?, ?, 100)</text>\n</g>\n<!-- 140268213922336&#45;&gt;140269074890824 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140268213922336-&gt;140269074890824</title>\n<path d=\"M138.5,-332.3799C138.5,-324.1745 138.5,-314.7679 138.5,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"142.0001,-305.784 138.5,-295.784 135.0001,-305.784 142.0001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140269334495360 -->\n<g class=\"node\" id=\"node3\">\n<title>140269334495360</title>\n<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 277,-212.5 277,-166.5 0,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"72\" y=\"-185.8\">AttentionWithContext</text>\n<polyline fill=\"none\" points=\"144,-166.5 144,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"173\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"144,-189.5 202,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"173\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"202,-166.5 202,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"239.5\" y=\"-197.3\">(?, ?, 100)</text>\n<polyline fill=\"none\" points=\"202,-189.5 277,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"239.5\" y=\"-174.3\">(?, ?, 100)</text>\n</g>\n<!-- 140269074890824&#45;&gt;140269334495360 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140269074890824-&gt;140269334495360</title>\n<path d=\"M138.5,-249.3799C138.5,-241.1745 138.5,-231.7679 138.5,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"142.0001,-222.784 138.5,-212.784 135.0001,-222.784 142.0001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140267999008304 -->\n<g class=\"node\" id=\"node4\">\n<title>140267999008304</title>\n<polygon fill=\"none\" points=\"38,-83.5 38,-129.5 239,-129.5 239,-83.5 38,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"72\" y=\"-102.8\">Addition</text>\n<polyline fill=\"none\" points=\"106,-83.5 106,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"106,-106.5 164,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"164,-83.5 164,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"201.5\" y=\"-114.3\">(?, ?, 100)</text>\n<polyline fill=\"none\" points=\"164,-106.5 239,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"201.5\" y=\"-91.3\">(?, 100)</text>\n</g>\n<!-- 140269334495360&#45;&gt;140267999008304 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140269334495360-&gt;140267999008304</title>\n<path d=\"M138.5,-166.3799C138.5,-158.1745 138.5,-148.7679 138.5,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"142.0001,-139.784 138.5,-129.784 135.0001,-139.784 142.0001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140269092665664 -->\n<g class=\"node\" id=\"node5\">\n<title>140269092665664</title>\n<polygon fill=\"none\" points=\"49,-.5 49,-46.5 228,-46.5 228,-.5 49,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"75\" y=\"-19.8\">Dense</text>\n<polyline fill=\"none\" points=\"101,-.5 101,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"130\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"101,-23.5 159,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"130\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"159,-.5 159,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"193.5\" y=\"-31.3\">(?, 100)</text>\n<polyline fill=\"none\" points=\"159,-23.5 228,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"193.5\" y=\"-8.3\">(?, 5799)</text>\n</g>\n<!-- 140267999008304&#45;&gt;140269092665664 -->\n<g class=\"edge\" id=\"edge4\">\n<title>140267999008304-&gt;140269092665664</title>\n<path d=\"M138.5,-83.3799C138.5,-75.1745 138.5,-65.7679 138.5,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"142.0001,-56.784 138.5,-46.784 135.0001,-56.784 142.0001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>",
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFiuxtGfUhwz",
        "outputId": "1df1c266-b865-4d05-9023-0ede13e5e238"
      },
      "source": [
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda\n",
        "cbow1 = Sequential()\n",
        "cbow1.add(Embedding(input_dim=vocab_size, output_dim=embed_size))\n",
        "\n",
        "cbow1.add(AttentionWithContext())\n",
        "# cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,0,0)))\n",
        "cbow1.add(Addition())\n",
        "cbow1.add(Dense(vocab_size, activation='softmax'))\n",
        "cbow1.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Line to load the model again\n",
        "cbow1.load_weights(\"/content/drive/MyDrive/ReverseDictionary/models/weights-improvement-cbow.hdf5\")\n",
        "weights = cbow1.get_weights()[0]\n",
        "weights = weights[1:]\n",
        "print(weights.shape)\n",
        "\n",
        "#pd.DataFrame(weights, index=list(id2word.values())[1:]).head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5798, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UepWh99ISNZH",
        "outputId": "7622ba43-e653-43f4-9ec5-54e3e55c38ef"
      },
      "source": [
        "# for epoch in range(1, 10):\n",
        "#     loss = 0.\n",
        "#     i = 0\n",
        "#     for x, y in generate_context_word_pairs1(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
        "#         i += 1\n",
        "#         loss += cbow1.train_on_batch(x, y)\n",
        "#         if i % 100000 == 0:\n",
        "#             print('Processed {} (context, word) pairs'.format(i))\n",
        "#     cbow1.save_weights(\"/content/drive/MyDrive/ReverseDictionary/models/weights-improvement-cbow.hdf5\",overwrite=True,save_format=None,options= None)\n",
        "#     print('Epoch:', epoch, '\\tLoss:', loss)\n",
        "#     print()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tLoss: 37.62212862759824\n",
            "\n",
            "Epoch: 2 \tLoss: 42.07611732387109\n",
            "\n",
            "Epoch: 3 \tLoss: 37.08370753236564\n",
            "\n",
            "Epoch: 4 \tLoss: 36.47604142263993\n",
            "\n",
            "Epoch: 5 \tLoss: 36.41550545354908\n",
            "\n",
            "Epoch: 6 \tLoss: 37.406807443795806\n",
            "\n",
            "Epoch: 7 \tLoss: 42.89909254739435\n",
            "\n",
            "Epoch: 8 \tLoss: 36.50969086641614\n",
            "\n",
            "Epoch: 9 \tLoss: 36.430974999089635\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4vRljvdUjYT",
        "outputId": "ed134918-3c99-4ff7-e51b-675f2001406e"
      },
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "# compute pairwise distance matrix\n",
        "distance_matrix = euclidean_distances(weights)\n",
        "print(distance_matrix.shape)\n",
        "\n",
        "# view contextually similar words\n",
        "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:11]+1] \n",
        "                   for search_term in ['abandon', 'about', 'absolute', 'accept', 'accurate', 'acid', 'acknowledge','actress']}\n",
        "\n",
        "similar_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5798, 5798)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'abandon': ['disloyal',\n",
              "  'embryo',\n",
              "  'compelling',\n",
              "  'salary',\n",
              "  'eagerness',\n",
              "  'nonexecutive',\n",
              "  'bath',\n",
              "  'relationships',\n",
              "  'birds',\n",
              "  'test'],\n",
              " 'about': ['comedy',\n",
              "  'she',\n",
              "  'myth',\n",
              "  'session',\n",
              "  'regardless',\n",
              "  'transform',\n",
              "  'editor',\n",
              "  'however',\n",
              "  'far',\n",
              "  'advise'],\n",
              " 'absolute': ['utmost',\n",
              "  'considerably',\n",
              "  'benefit',\n",
              "  'regulate',\n",
              "  'each',\n",
              "  'troop',\n",
              "  'sorry',\n",
              "  'reveal',\n",
              "  'virtue',\n",
              "  'frame'],\n",
              " 'accept': ['measured',\n",
              "  'iron',\n",
              "  'white',\n",
              "  'electrical',\n",
              "  'wrong',\n",
              "  'traditionally',\n",
              "  'reflects',\n",
              "  'sold',\n",
              "  'spread',\n",
              "  'rope'],\n",
              " 'accurate': ['granular',\n",
              "  'yellowish',\n",
              "  'rocks',\n",
              "  'beds',\n",
              "  'beaches',\n",
              "  'deserts',\n",
              "  'offer',\n",
              "  'seabed',\n",
              "  'roughly',\n",
              "  'distinguishing'],\n",
              " 'acid': ['microscopy',\n",
              "  'nucleic',\n",
              "  'multiply',\n",
              "  'molecule',\n",
              "  'coat',\n",
              "  'infective',\n",
              "  'host',\n",
              "  'toilet',\n",
              "  'bath',\n",
              "  'washbasin'],\n",
              " 'acknowledge': ['intentionally',\n",
              "  'disregard',\n",
              "  'relationships',\n",
              "  'release',\n",
              "  'maintains',\n",
              "  'entertains',\n",
              "  'observance',\n",
              "  'deriving',\n",
              "  'nonexecutive',\n",
              "  'solicitor'],\n",
              " 'actress': ['impressive',\n",
              "  'apartment',\n",
              "  'league',\n",
              "  'slip',\n",
              "  'helpful',\n",
              "  'invasion',\n",
              "  'involve',\n",
              "  'nod',\n",
              "  'chest',\n",
              "  'dance']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhb67TTWc9qn"
      },
      "source": [
        "import numpy as np\n",
        "def evaluate(ground_truth, prediction):\n",
        "   accu_1 = 0.\n",
        "   accu_10 = 0.\n",
        "   accu_100 = 0.\n",
        "   length = len(ground_truth)\n",
        "   for i in range(length):\n",
        "       if ground_truth[i] in prediction[i][:100]:\n",
        "           accu_100 += 1\n",
        "           if ground_truth[i] in prediction[i][:10]:\n",
        "               accu_10 += 1\n",
        "               if ground_truth[i] == prediction[i][0]:\n",
        "                   accu_1 += 1\n",
        "   return accu_1/length*100, accu_10/length*100, accu_100/length*100\n",
        "\n",
        "def evaluate_test(ground_truth, prediction):\n",
        "   # print(ground_truth)\n",
        "   # print(prediction)\n",
        "   accu_1 = 0.\n",
        "   accu_10 = 0.\n",
        "   accu_100 = 0.\n",
        "   length = len(ground_truth)\n",
        "   pred_rank = []\n",
        "   for i in range(length):\n",
        "       try:\n",
        "           pred_rank.append(prediction[i].tolist().index(ground_truth[i]))\n",
        "       except:\n",
        "           pred_rank.append(1000)\n",
        "       if ground_truth[i] in prediction[i][:100]:\n",
        "           accu_100 += 1\n",
        "           if ground_truth[i] in prediction[i][:10]:\n",
        "               accu_10 += 1\n",
        "               if ground_truth[i] == prediction[i][0]:\n",
        "                   accu_1 += 1\n",
        "   print(pred_rank)\n",
        "   return accu_1/length*100, accu_10/length*100, accu_100/length*100, np.median(pred_rank), np.sqrt(np.var(pred_rank))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkICDU8FIlR4",
        "outputId": "b7d4ec25-3187-42bf-e598-169dbde26fa5"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "predictions = []\n",
        "ground_truth = []\n",
        "#test = open('/content/drive/MyDrive/ReverseDictionary/data/dataset_final.txt', 'r')\n",
        "test = open('/content/drive/MyDrive/ReverseDictionary/data/dataset9.txt', 'r')\n",
        "for i, line in enumerate(test):\n",
        "    print(line)\n",
        "    if(i==200):\n",
        "      break\n",
        "    else:\n",
        "      print(\"Testing \" + str(i) + \"/100\")\n",
        "    line = line.strip('\\n').split('  ')\n",
        "    # if(line[0] not in idx2word):\n",
        "    #   continue\n",
        "    ground_truth.append(word2id[line[0]])\n",
        "    # print(ground_truth)\n",
        "    definition = list(set(line[1].split(' ')))        \t\n",
        "    stopwords = ['a','an','the','is','are','in','of','by','at']\n",
        "    if(len(definition)>18):\n",
        "      for x in stopwords:\n",
        "        if x in definition:\n",
        "          definition.remove(x)\n",
        "    idxs = []\n",
        "    for word in definition:\n",
        "        if(word==\"\"):\n",
        "          continue\n",
        "        idxs.append(word2id[word])\n",
        "    idxs = np.array([0] + idxs + [1]).reshape((1,len(idxs) + 2))\n",
        "    #idxs = np.pad(np.array([0] + idxs + [1]), (0,l), 'constant', constant_values=(0))\n",
        "    #idxs = idxs.reshape((1,20)) \n",
        "    prediction = cbow1.predict(idxs, verbose=0)\n",
        "    ind=np.argpartition(prediction[0],-100)[-100:]\n",
        "    predictions.append(ind)\n",
        "    # print(predictions)\n",
        "\n",
        "A_1,A_10,A_100,med,sq = evaluate_test(ground_truth,predictions)\n",
        "print(\"Accuracy @ 1 = \"+ str(A_1))\n",
        "print(\"Accuracy @ 10 = \"+ str(A_10))\n",
        "print(\"Accuracy @ 100 = \"+ str(A_100))\n",
        "print(\"Median Prediction Rank = \"+ str(med))\n",
        "# print(\"Accuracy @ 1 = \"+ A_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "abandon   cease support look someone desert\n",
            "\n",
            "Testing 0/100\n",
            "ability   possession means skill something\n",
            "\n",
            "Testing 1/100\n",
            "able   power skill means opportunity something\n",
            "\n",
            "Testing 2/100\n",
            "abortion   deliberate termination human pregnancy often performed first weeks pregnancy\n",
            "\n",
            "Testing 3/100\n",
            "about   subject concerning\n",
            "\n",
            "Testing 4/100\n",
            "above   extended space touching\n",
            "\n",
            "Testing 5/100\n",
            "abroad   foreign country countries\n",
            "\n",
            "Testing 6/100\n",
            "absence   state away place person\n",
            "\n",
            "Testing 7/100\n",
            "absolute   qualified diminished way total\n",
            "\n",
            "Testing 8/100\n",
            "absolutely   qualification restriction limitation totally\n",
            "\n",
            "Testing 9/100\n",
            "absorb   take soak energy liquid substance chemical physical action\n",
            "\n",
            "Testing 10/100\n",
            "abuse   something effect purpose misuse\n",
            "\n",
            "Testing 11/100\n",
            "academic   relating education scholarship\n",
            "\n",
            "Testing 12/100\n",
            "accept   consent receive undertake something offered\n",
            "\n",
            "Testing 13/100\n",
            "access   means opportunity approach enter place\n",
            "\n",
            "Testing 14/100\n",
            "accident   unfortunate incident happens unexpectedly unintentionally typically resulting damage injury\n",
            "\n",
            "Testing 15/100\n",
            "accompany   somewhere someone companion escort\n",
            "\n",
            "Testing 16/100\n",
            "accomplish   achieve complete successfully\n",
            "\n",
            "Testing 17/100\n",
            "according   stated\n",
            "\n",
            "Testing 18/100\n",
            "account   report description event experience\n",
            "\n",
            "Testing 19/100\n",
            "accurate   especially information measurements predictions correct details exact\n",
            "\n",
            "Testing 20/100\n",
            "accuse   charge someone offence crime\n",
            "\n",
            "Testing 21/100\n",
            "achieve   successfully bring reach desired objective result effort skill courage\n",
            "\n",
            "Testing 22/100\n",
            "achievement   thing done successfully effort skill courage\n",
            "\n",
            "Testing 23/100\n",
            "acid   substance particular chemical properties including turning litmus red neutralizing alkalis dissolving metals typically corrosive sourtasting liquid kind\n",
            "\n",
            "Testing 24/100\n",
            "acknowledge   accept admit existence truth\n",
            "\n",
            "Testing 25/100\n",
            "acquire   obtain asset object oneself\n",
            "\n",
            "Testing 26/100\n",
            "across   side place area etc\n",
            "\n",
            "Testing 27/100\n",
            "act   take action something\n",
            "\n",
            "Testing 28/100\n",
            "action   fact process something typically achieve aim\n",
            "\n",
            "Testing 29/100\n",
            "active   engaging ready engage physically energetic pursuits\n",
            "\n",
            "Testing 30/100\n",
            "activist   person campaigns bring political social change\n",
            "\n",
            "Testing 31/100\n",
            "activity   condition things happening done\n",
            "\n",
            "Testing 32/100\n",
            "actor   person whose profession acting stage films television\n",
            "\n",
            "Testing 33/100\n",
            "actress   woman whose profession acting stage films television\n",
            "\n",
            "Testing 34/100\n",
            "actual   existing fact real\n",
            "\n",
            "Testing 35/100\n",
            "actually   truth facts situation really\n",
            "\n",
            "Testing 36/100\n",
            "adapt   make something suitable purpose modify\n",
            "\n",
            "Testing 37/100\n",
            "add   join something something else increase size number amount\n",
            "\n",
            "Testing 38/100\n",
            "addition   action process adding something something else\n",
            "\n",
            "Testing 39/100\n",
            "additional   added extra supplementary already present available\n",
            "\n",
            "Testing 40/100\n",
            "address   particulars place someone lives organization situated\n",
            "\n",
            "Testing 41/100\n",
            "adequate   satisfactory acceptable quality quantity\n",
            "\n",
            "Testing 42/100\n",
            "adjust   alter move something slightly order achieve desired fit appearance result\n",
            "\n",
            "Testing 43/100\n",
            "adjustment   small alteration movement made achieve desired fit appearance result\n",
            "\n",
            "Testing 44/100\n",
            "administration   process activity running business organization etc\n",
            "\n",
            "Testing 45/100\n",
            "administrator   person responsible carrying administration business organization\n",
            "\n",
            "Testing 46/100\n",
            "admire   regard respect warm approval\n",
            "\n",
            "Testing 47/100\n",
            "admission   statement acknowledging truth something\n",
            "\n",
            "Testing 48/100\n",
            "admit   confess true case\n",
            "\n",
            "Testing 49/100\n",
            "adolescent   young person process developing child adult\n",
            "\n",
            "Testing 50/100\n",
            "adopt   legally take anothers child bring ones own\n",
            "\n",
            "Testing 51/100\n",
            "adult   person fully grown developed\n",
            "\n",
            "Testing 52/100\n",
            "advance   move forwards purposeful way\n",
            "\n",
            "Testing 53/100\n",
            "advanced   ahead development progress\n",
            "\n",
            "Testing 54/100\n",
            "advantage   condition circumstance puts favourable superior position\n",
            "\n",
            "Testing 55/100\n",
            "adventure   unusual exciting daring experience\n",
            "\n",
            "Testing 56/100\n",
            "advertising   activity profession producing advertisements commercial products services\n",
            "\n",
            "Testing 57/100\n",
            "advice   guidance recommendations offered regard prudent future action\n",
            "\n",
            "Testing 58/100\n",
            "advise   offer suggestions best course action someone\n",
            "\n",
            "Testing 59/100\n",
            "adviser   person gives advice particular field\n",
            "\n",
            "Testing 60/100\n",
            "advocate   person publicly supports recommends particular cause policy\n",
            "\n",
            "Testing 61/100\n",
            "affair   event sequence events specified kind previously referred\n",
            "\n",
            "Testing 62/100\n",
            "affect   effect make difference\n",
            "\n",
            "Testing 63/100\n",
            "afford   enough money for\n",
            "\n",
            "Testing 64/100\n",
            "afraid   feeling fear anxiety frightened\n",
            "\n",
            "Testing 65/100\n",
            "after   time following event another period time\n",
            "\n",
            "Testing 66/100\n",
            "afternoon   time noon lunchtime evening\n",
            "\n",
            "Testing 67/100\n",
            "again   another time more\n",
            "\n",
            "Testing 68/100\n",
            "against   opposition\n",
            "\n",
            "Testing 69/100\n",
            "age   length time person lived thing existed\n",
            "\n",
            "Testing 70/100\n",
            "agency   business organization providing particular service behalf another business person group\n",
            "\n",
            "Testing 71/100\n",
            "agenda   list items discussed formal meeting\n",
            "\n",
            "Testing 72/100\n",
            "agent   person acts behalf another person group\n",
            "\n",
            "Testing 73/100\n",
            "aggressive   ready likely attack confront characterized resulting aggression\n",
            "\n",
            "Testing 74/100\n",
            "ago   present earlier used measurement time\n",
            "\n",
            "Testing 75/100\n",
            "agree   opinion something concur\n",
            "\n",
            "Testing 76/100\n",
            "agreement   harmony accordance opinion feeling\n",
            "\n",
            "Testing 77/100\n",
            "agricultural   relating agriculture\n",
            "\n",
            "Testing 78/100\n",
            "used   express range emotions including surprise pleasure sympathy realization\n",
            "\n",
            "Testing 79/100\n",
            "ahead   forward space line ones forward motion\n",
            "\n",
            "Testing 80/100\n",
            "aid   help typically practical nature\n",
            "\n",
            "Testing 81/100\n",
            "aide   assistant important person especially political leader\n",
            "\n",
            "Testing 82/100\n",
            "aim   point direct weapon camera target\n",
            "\n",
            "Testing 83/100\n",
            "air   invisible gaseous substance surrounding earth mixture mainly oxygen nitrogen\n",
            "\n",
            "Testing 84/100\n",
            "aircraft   aeroplane helicopter machine capable flight\n",
            "\n",
            "Testing 85/100\n",
            "airline   organization providing regular public service transport routes\n",
            "\n",
            "Testing 86/100\n",
            "airport   complex runways buildings takeoff landing maintenance civil aircraft facilities passengers\n",
            "\n",
            "Testing 87/100\n",
            "album   blank book insertion photographs stamps pictures\n",
            "\n",
            "Testing 88/100\n",
            "alcohol   colourless volatile flammable liquid produced natural fermentation sugars intoxicating constituent wine beer spirits drinks also used industrial solvent fuel\n",
            "\n",
            "Testing 89/100\n",
            "alive   person animal plant living dead\n",
            "\n",
            "Testing 90/100\n",
            "all   used refer whole quantity extent particular group thing\n",
            "\n",
            "Testing 91/100\n",
            "alliance   union association formed mutual benefit especially countries organizations\n",
            "\n",
            "Testing 92/100\n",
            "allow   someone something\n",
            "\n",
            "Testing 93/100\n",
            "ally   state formally cooperating another military purpose\n",
            "\n",
            "Testing 94/100\n",
            "almost   quite nearly\n",
            "\n",
            "Testing 95/100\n",
            "alone   else present ones own\n",
            "\n",
            "Testing 96/100\n",
            "along   moving constant direction less horizontal surface\n",
            "\n",
            "Testing 97/100\n",
            "already   time question\n",
            "\n",
            "Testing 98/100\n",
            "also   addition too\n",
            "\n",
            "Testing 99/100\n",
            "alter   change character composition typically comparatively small significant way\n",
            "\n",
            "Testing 100/100\n",
            "alternative   things available another possibility choice\n",
            "\n",
            "Testing 101/100\n",
            "although   spite fact that even though\n",
            "\n",
            "Testing 102/100\n",
            "always   times occasions\n",
            "\n",
            "Testing 103/100\n",
            "amazing   causing great surprise wonder astonishing\n",
            "\n",
            "Testing 104/100\n",
            "among   situated less centrally relation several things\n",
            "\n",
            "Testing 105/100\n",
            "amount   quantity something especially total thing things number size value extent\n",
            "\n",
            "Testing 106/100\n",
            "analysis   detailed examination elements structure something\n",
            "\n",
            "Testing 107/100\n",
            "analyst   person conducts analysis\n",
            "\n",
            "Testing 108/100\n",
            "analyze   examine something methodically detail typically order explain interpret\n",
            "\n",
            "Testing 109/100\n",
            "ancient   belonging distant past longer existence\n",
            "\n",
            "Testing 110/100\n",
            "and   used connect words part speech clauses sentences taken jointly\n",
            "\n",
            "Testing 111/100\n",
            "anger   strong feeling annoyance displeasure hostility\n",
            "\n",
            "Testing 112/100\n",
            "angle   space usually measured degrees intersecting lines surfaces close point meet\n",
            "\n",
            "Testing 113/100\n",
            "angry   feeling showing strong annoyance displeasure hostility full anger\n",
            "\n",
            "Testing 114/100\n",
            "animal   living organism feeds organic matter typically specialized sense organs nervous system able respond rapidly stimuli\n",
            "\n",
            "Testing 115/100\n",
            "anniversary   date event took place institution founded previous year\n",
            "\n",
            "Testing 116/100\n",
            "announce   make formal public statement fact occurrence intention\n",
            "\n",
            "Testing 117/100\n",
            "annual   occurring every year\n",
            "\n",
            "Testing 118/100\n",
            "another   used refer additional person thing type already mentioned known about more further\n",
            "\n",
            "Testing 119/100\n",
            "answer   thing said written done reaction question statement situation\n",
            "\n",
            "Testing 120/100\n",
            "anticipate   regard probable expect predict\n",
            "\n",
            "Testing 121/100\n",
            "anxiety   feeling worry nervousness unease something uncertain outcome\n",
            "\n",
            "Testing 122/100\n",
            "any   used refer thing number things matter much many\n",
            "\n",
            "Testing 123/100\n",
            "anybody   anyone\n",
            "\n",
            "Testing 124/100\n",
            "anymore   extent longer\n",
            "\n",
            "Testing 125/100\n",
            "anyone   person people\n",
            "\n",
            "Testing 126/100\n",
            "anything   used refer thing matter what\n",
            "\n",
            "Testing 127/100\n",
            "anyway   used confirm support point idea mentioned\n",
            "\n",
            "Testing 128/100\n",
            "anywhere   place\n",
            "\n",
            "Testing 129/100\n",
            "apart   people things separated specified distance time space\n",
            "\n",
            "Testing 130/100\n",
            "apartment   flat typically well appointed used holidays\n",
            "\n",
            "Testing 131/100\n",
            "apparent   clearly visible understood obvious\n",
            "\n",
            "Testing 132/100\n",
            "apparently   knows see\n",
            "\n",
            "Testing 133/100\n",
            "appeal   make serious urgent heartfelt request\n",
            "\n",
            "Testing 134/100\n",
            "appear   come sight become visible noticeable especially without apparent cause\n",
            "\n",
            "Testing 135/100\n",
            "appearance   someone something looks\n",
            "\n",
            "Testing 136/100\n",
            "apple   round fruit tree rose family typically thin green skin crisp flesh\n",
            "\n",
            "Testing 137/100\n",
            "application   formal request considered position allowed something submitted authority institution organization\n",
            "\n",
            "Testing 138/100\n",
            "apply   make formal application request\n",
            "\n",
            "Testing 139/100\n",
            "appoint   assign role someone\n",
            "\n",
            "Testing 140/100\n",
            "appointment   arrangement meet someone particular time place\n",
            "\n",
            "Testing 141/100\n",
            "appreciate   recognize full worth\n",
            "\n",
            "Testing 142/100\n",
            "approach   come near nearer someone something distance time\n",
            "\n",
            "Testing 143/100\n",
            "appropriate   suitable proper circumstances\n",
            "\n",
            "Testing 144/100\n",
            "approval   action approving something\n",
            "\n",
            "Testing 145/100\n",
            "approve   officially agree accept satisfactory\n",
            "\n",
            "Testing 146/100\n",
            "approximately   used show something almost completely accurate exact roughly\n",
            "\n",
            "Testing 147/100\n",
            "architect   person designs buildings many cases also supervises construction\n",
            "\n",
            "Testing 148/100\n",
            "area   region part town country world\n",
            "\n",
            "Testing 149/100\n",
            "argue   give reasons cite evidence support idea action theory typically persuading others share ones view\n",
            "\n",
            "Testing 150/100\n",
            "argument   exchange diverging opposite views typically heated angry one\n",
            "\n",
            "Testing 151/100\n",
            "arise   problem opportunity situation emerge become apparent\n",
            "\n",
            "Testing 152/100\n",
            "arm   upper limbs human body shoulder hand\n",
            "\n",
            "Testing 153/100\n",
            "armed   equipped carrying firearm firearms\n",
            "\n",
            "Testing 154/100\n",
            "army   organized military force equipped fighting land\n",
            "\n",
            "Testing 155/100\n",
            "around   located situated every side\n",
            "\n",
            "Testing 156/100\n",
            "arrange   things neat attractive required order\n",
            "\n",
            "Testing 157/100\n",
            "arrangement   action process result arranging arranged\n",
            "\n",
            "Testing 158/100\n",
            "arrest   seize someone legal authority take custody\n",
            "\n",
            "Testing 159/100\n",
            "arrival   action process arriving\n",
            "\n",
            "Testing 160/100\n",
            "arrive   reach place journey stage journey\n",
            "\n",
            "Testing 161/100\n",
            "art   expression application human creative skill imagination typically visual form painting sculpture producing works appreciated primarily beauty emotional power\n",
            "\n",
            "Testing 162/100\n",
            "article   particular item object\n",
            "\n",
            "Testing 163/100\n",
            "artist   person creates paintings drawings profession hobby\n",
            "\n",
            "Testing 164/100\n",
            "artistic   revealing natural creative skill\n",
            "\n",
            "Testing 165/100\n",
            "aside   side way\n",
            "\n",
            "Testing 166/100\n",
            "ask   something order obtain answer information\n",
            "\n",
            "Testing 167/100\n",
            "asleep   state sleep\n",
            "\n",
            "Testing 168/100\n",
            "aspect   particular part feature something\n",
            "\n",
            "Testing 169/100\n",
            "assault   make physical attack\n",
            "\n",
            "Testing 170/100\n",
            "assert   state fact belief confidently forcefully\n",
            "\n",
            "Testing 171/100\n",
            "assess   evaluate estimate nature ability quality\n",
            "\n",
            "Testing 172/100\n",
            "assessment   action assessing someone something\n",
            "\n",
            "Testing 173/100\n",
            "asset   useful valuable thing person\n",
            "\n",
            "Testing 174/100\n",
            "assign   allocate duty\n",
            "\n",
            "Testing 175/100\n",
            "assignment   task piece work allocated someone part course study\n",
            "\n",
            "Testing 176/100\n",
            "assist   help someone typically share work\n",
            "\n",
            "Testing 177/100\n",
            "assistance   action helping someone sharing work\n",
            "\n",
            "Testing 178/100\n",
            "assistant   person ranks senior person\n",
            "\n",
            "Testing 179/100\n",
            "associate   connect someone something something else ones mind\n",
            "\n",
            "Testing 180/100\n",
            "association   often names group people organized joint purpose\n",
            "\n",
            "Testing 181/100\n",
            "assume   suppose case without proof\n",
            "\n",
            "Testing 182/100\n",
            "assumption   thing accepted true certain happen without proof\n",
            "\n",
            "Testing 183/100\n",
            "assure   tell someone something positively dispel doubts\n",
            "\n",
            "Testing 184/100\n",
            "expressing   location arrival particular place position\n",
            "\n",
            "Testing 185/100\n",
            "athlete   person proficient sports forms physical exercise\n",
            "\n",
            "Testing 186/100\n",
            "athletic   physically strong fit active\n",
            "\n",
            "Testing 187/100\n",
            "atmosphere   envelope gases surrounding earth another planet\n",
            "\n",
            "Testing 188/100\n",
            "attach   join fasten something something else\n",
            "\n",
            "Testing 189/100\n",
            "attack   take aggressive military action place enemy forces weapons armed force\n",
            "\n",
            "Testing 190/100\n",
            "attempt   make effort achieve complete something difficult\n",
            "\n",
            "Testing 191/100\n",
            "attend   present event meeting function\n",
            "\n",
            "Testing 192/100\n",
            "attention   notice taken someone something regarding someone something interesting important\n",
            "\n",
            "Testing 193/100\n",
            "attitude   settled thinking feeling something\n",
            "\n",
            "Testing 194/100\n",
            "attorney   person typically lawyer appointed another business legal matters\n",
            "\n",
            "Testing 195/100\n",
            "attract   cause come place participate venture offering something interest advantage\n",
            "\n",
            "Testing 196/100\n",
            "attractive   pleasing appealing senses\n",
            "\n",
            "Testing 197/100\n",
            "attribute   regard something caused\n",
            "\n",
            "Testing 198/100\n",
            "audience   assembled spectators listeners public event play film concert meeting\n",
            "\n",
            "Testing 199/100\n",
            "author   writer book article document\n",
            "\n",
            "[49, 27, 62, 66, 8, 62, 44, 1000, 35, 92, 65, 89, 66, 46, 84, 55, 92, 49, 91, 29, 71, 42, 9, 68, 49, 78, 54, 1000, 1000, 93, 99, 19, 80, 46, 82, 87, 72, 55, 73, 98, 63, 65, 94, 44, 87, 92, 83, 94, 37, 93, 89, 73, 74, 63, 90, 71, 84, 79, 63, 98, 52, 77, 46, 43, 24, 52, 75, 42, 37, 1000, 68, 34, 52, 58, 83, 85, 94, 68, 72, 98, 61, 45, 99, 82, 80, 97, 35, 67, 44, 65, 98, 89, 78, 72, 95, 26, 55, 46, 34, 97, 61, 75, 99, 82, 74, 49, 73, 74, 37, 91, 48, 67, 61, 75, 57, 83, 56, 49, 51, 93, 97, 63, 38, 44, 27, 1000, 34, 90, 36, 63, 73, 67, 54, 77, 73, 73, 63, 69, 13, 81, 94, 62, 43, 39, 38, 31, 35, 60, 70, 9, 81, 94, 43, 41, 98, 48, 68, 38, 44, 43, 52, 49, 53, 25, 72, 33, 59, 56, 20, 85, 50, 91, 59, 90, 90, 40, 44, 73, 68, 77, 72, 21, 79, 28, 46, 86, 98, 88, 40, 40, 40, 87, 94, 39, 45, 82, 80, 39, 58, 72]\n",
            "Accuracy @ 1 = 0.0\n",
            "Accuracy @ 10 = 1.5\n",
            "Accuracy @ 100 = 97.5\n",
            "Median Prediction Rank = 66.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2su5p1OU3POg",
        "outputId": "6b4043c1-f8d3-4dc9-94de-103cbeb39627"
      },
      "source": [
        "!pip install flask-ngrok\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, request, jsonify, render_template, logging \n",
        "import webbrowser "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (1.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJRc0drn3R-Q",
        "outputId": "d50584f3-e93d-4226-8ab1-f568ed3e386c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "def create_app():\n",
        "    app = Flask(__name__, template_folder='/content/drive/MyDrive/Colab Notebooks/templates', static_folder='/content/drive/MyDrive/Colab Notebooks/static')  \n",
        "    return app\n",
        "    \n",
        "app = create_app()\n",
        "\n",
        "#WEB-APP\n",
        "@app.route('/', methods=['GET', 'POST'])\n",
        "def index():\n",
        "    meaning=[]\n",
        "    if request.method == 'POST':\n",
        "        words1 = request.form['definition'].split() \n",
        "        words2=[]\n",
        "        for word in words1:\n",
        "            if len(word)>=4 and word not in stopwords.words('english'):\n",
        "                words2.append(word) \n",
        "        idxs = []\n",
        "        for word in words2:\n",
        "            idxs.append(word2id[word])\n",
        "\n",
        "        idxs = np.array([0] + idxs + [1]).reshape((1,len(idxs) + 2))\n",
        "        prediction = cbow1.predict(idxs, verbose=0)\n",
        "        #print(prediction)\n",
        "        ind=np.argpartition(prediction[0],-10)[-10:]\n",
        "        #print(ind)\n",
        "        for i in ind:\n",
        "            meaning.append(id2word[i])\n",
        "        return jsonify(meaning)\n",
        "        #for idx in distance_matrix[word2id[words]-1].argsort()[1:6]+1:\n",
        "        #    meaning.append(id2word[idx])\n",
        "        #return jsonify(meaning)        \n",
        "    else:\n",
        "        #return \"<h1>Running Flask on Google Colab!</h1>\"\n",
        "        return render_template('index.html')\n",
        "\n",
        "\n",
        "run_with_ngrok(app) \n",
        "app.run()\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://880834cf630c.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [07/Dec/2020 04:22:49] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [07/Dec/2020 04:22:52] \"\u001b[37mGET /static/js/jquery-3.3.1.min.js HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [07/Dec/2020 04:22:52] \"\u001b[37mGET /static/js/main.js HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [07/Dec/2020 04:22:52] \"\u001b[37mGET /static/css/bulma/bulma.min.css HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [07/Dec/2020 04:22:55] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [07/Dec/2020 04:43:47] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [07/Dec/2020 04:43:50] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [07/Dec/2020 04:43:54] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [07/Dec/2020 05:58:05] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [07/Dec/2020 05:58:41] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh6B3AUJauOz",
        "outputId": "7be64eb2-0bea-473a-b3d9-78a48955efd9"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "definition = input().lower()\n",
        "#print(definition)\n",
        "words1 = definition.split(' ')\n",
        "words2=[]\n",
        "for word in words1:\n",
        "  if len(word)>=4 and word not in stopwords.words('english'):\n",
        "    words2.append(word)\n",
        "#words = definition.split(' ')\n",
        "idxs = []\n",
        "for word in words2:\n",
        "    idxs.append(word2id[word])\n",
        "\n",
        "idxs = np.array([0] + idxs + [1]).reshape((1,len(idxs) + 2))\n",
        "prediction = cbow1.predict(idxs, verbose=0)\n",
        "#print(prediction)\n",
        "# index = np.argmax(prediction)\n",
        "ind=np.argpartition(prediction[0],-10)[-10:]\n",
        "#print(ind)\n",
        "for i in ind:\n",
        "  # meaning = idx2word[index]\n",
        "  meaning = id2word[i]\n",
        "  print(meaning)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "similar to happiness\n",
            "awareness\n",
            "aware\n",
            "depending\n",
            "change\n",
            "always\n",
            "changing\n",
            "depend\n",
            "already\n",
            "deeply\n",
            "indoeuropean\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}